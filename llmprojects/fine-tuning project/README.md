# Qu√° tr√¨nh Fine-tuning cho d·ª± √°n n√†y

# 1. Dataset
   Ch√∫ng ta s·ª≠ d·ª•ng b·ªô d·ªØ li·ªáu t·ª´ Hugging Face:
üëâ [Amazon-Reviews-2023-Books-Meta](https://huggingface.co/datasets/cogsci13/Amazon-Reviews-2023-Books-Meta)

# 2. Data Preparation Process
   Quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu g·ªìm 4 b∆∞·ªõc:

### Data Cleaning

   Lo·∫°i b·ªè c√°c th√¥ng tin kh√¥ng li√™n quan nh∆∞ m√£ s·∫£n ph·∫©m, k√Ω t·ª± ƒë·∫∑c bi·ªát v√† c√°c tr∆∞·ªùng th√¥ng tin kh√¥ng gi√∫p √≠ch cho vi·ªác d·ª± ƒëo√°n gi√°.

### Tokenization

   S·ª≠ d·ª•ng tokenizer c·ªßa m√¥ h√¨nh Llama ƒë·ªÉ ƒë·∫øm v√† gi·ªõi h·∫°n s·ªë l∆∞·ª£ng token trong m·ªói v√≠ d·ª•.

### Quality Filtering

   Ch·ªâ gi·ªØ l·∫°i nh·ªØng m·∫´u d·ªØ li·ªáu:
   
   ƒê·ªß n·ªôi dung h·ªØu √≠ch (h∆°n 150 tokens).
   
   Kh√¥ng qu√° d√†i (d∆∞·ªõi 160 tokens).

### Prompt Standardization

   Chu·∫©n h√≥a t·∫•t c·∫£ c√°c prompt theo c·∫•u tr√∫c:
   
       **Question: "How much does this cost to the nearest dollar?"**
       **Content: (n·ªôi dung s√°ch)**
       **Answer: (gi√°)**

# 3. Machine Learning Modeling (Before Fine-tuning)

   - Random model: Sai s·ªë: $423.71 (r·∫•t k√©m).
   
   - Linear Regression: Sai s·ªë gi·∫£m c√≤n $52.94.
   
   - Word2Vec: Sai s·ªë c√≤n $50.94.
   
   - HistGradientBoosting: Sai s·ªë th·∫•p nh·∫•t $50.74.

# 4. Fine-tuning Techniques

   LoRA (Low-Rank Adaptation):
   T√†i li·ªáu: [LoRA Guide - Huggingface](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)
   
   QLoRA (Quantized LoRA):
   T√†i li·ªáu: [QLoRA Guide - Huggingface](https://huggingface.co/docs/peft/main/en/developer_guides/quantization)
   
   Paper tham kh·∫£o:
   üìÑ [QLoRA Paper (2023)](https://arxiv.org/pdf/2305.14314)

# 5. Important Hyperparameters

### 5.1 R (Rank)

   √ù nghƒ©a: X√°c ƒë·ªãnh k√≠ch th∆∞·ªõc kh√¥ng gian con trong LoRA (s·ªë l∆∞·ª£ng tham s·ªë th·∫•p h∆°n ƒë∆∞·ª£c hu·∫•n luy·ªán).
   
   Gi√° tr·ªã c√†ng nh·ªè ‚Üí √≠t tham s·ªë c·∫ßn c·∫≠p nh·∫≠t ‚Üí nhanh h∆°n, nh∆∞ng c√≥ th·ªÉ gi·∫£m hi·ªáu qu·∫£ fine-tuning.
   
   Docs: [PEFT LoRA Parameters](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)

### 5.2 Alpha

   √ù nghƒ©a: H·ªá s·ªë scale khu·∫øch ƒë·∫°i ·∫£nh h∆∞·ªüng c·ªßa c√°c tham s·ªë LoRA l√™n tr·ªçng s·ªë ban ƒë·∫ßu.
   
   Alpha c√†ng l·ªõn ‚Üí ·∫£nh h∆∞·ªüng LoRA c√†ng m·∫°nh m·∫Ω.
   
   Docs: [LoRA Alpha Scaling (g·ªëc LoRA paper).](https://arxiv.org/pdf/2106.09685.pdf)

### 5.3 Target Modules

   √ù nghƒ©a: C√°c module trong m√¥ h√¨nh m√† LoRA s·∫Ω √°p d·ª•ng.
   
   V√≠ d·ª•: "q_proj", "k_proj", "v_proj", "o_proj" (c√°c l·ªõp attention projection).
   
   Docs: [Choosing Target Modules](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora#how-lora-works)

# 6. LLMs Training Process

### 6.1 Forward Pass

   ƒê∆∞a d·ªØ li·ªáu ƒë·∫ßu v√†o qua m√¥ h√¨nh ƒë·ªÉ m√¥ h√¨nh d·ª± ƒëo√°n token ti·∫øp theo.

### 6.2 Loss Calculation

   So s√°nh ƒë·∫ßu ra c·ªßa m√¥ h√¨nh v·ªõi nh√£n th·ª±c t·∫ø ƒë·ªÉ t√≠nh to√°n loss.

### 6.3 Backward Pass

   T√≠nh to√°n gradient c·ªßa loss v·ªõi respect t·ªõi c√°c tham s·ªë m√¥ h√¨nh.

### 6.4 Optimization

   C·∫≠p nh·∫≠t tr·ªçng s·ªë theo c√¥ng th·ª©c:

**new_weight = old_weight - learning_rate * gradient**

üìö Docs: [Deep Learning Book - Optimization](https://www.deeplearningbook.org/contents/numerical.html)

# 7. Training Configuration

### 7.1 SFTTrainer Configuration

   Epochs: S·ªë l·∫ßn duy·ªát to√†n b·ªô t·∫≠p d·ªØ li·ªáu hu·∫•n luy·ªán (c√≥ th·ªÉ tƒÉng l√™n 4+).
   
   Batch size: 1 (per device) √ó 16 (gradient accumulation) = 16 m·∫´u/l·∫ßn update.

### 7.2 Optimizer

   paged_adamw_32bit:
   
   Phi√™n b·∫£n AdamW t·ªëi ∆∞u b·ªô nh·ªõ khi fine-tune m√¥ h√¨nh l·ªõn.
   
   T√†i li·ªáu: [PagedAdamW Optimizer](https://huggingface.co/docs/bitsandbytes/main/en/reference/optim/adamw)

### 7.3 Learning Rate

   Tham s·ªë quy·∫øt ƒë·ªãnh t·ªëc ƒë·ªô c·∫≠p nh·∫≠t tr·ªçng s·ªë.
   
   N·∫øu qu√° cao ‚Üí c√≥ th·ªÉ nh·∫£y kh·ªèi ƒëi·ªÉm t·ªëi ∆∞u.
   
   N·∫øu qu√° th·∫•p ‚Üí m√¥ h√¨nh h·ªçc r·∫•t ch·∫≠m ho·∫∑c k·∫πt local minimum.

### 7.4 Scheduler

   Cosine scheduler:
   
   Learning rate gi·∫£m theo h√¨nh d·∫°ng s√≥ng cosine.
   
   B·∫Øt ƒë·∫ßu gi·∫£m nh·∫π ‚Üí gi·∫£m m·∫°nh ‚Üí ·ªïn ƒë·ªãnh d·∫ßn.
   
   Docs: [Cosine LR Scheduler](https://discuss.huggingface.co/t/using-cosine-lr-scheduler-via-trainingarguments-in-trainer/14783/6)

### 7.5 Warmup Ratio

   Giai ƒëo·∫°n kh·ªüi ƒë·ªông, learning rate tƒÉng d·∫ßn t·ª´ th·∫•p ƒë·∫øn m·ª©c cao nh·∫•t r·ªìi m·ªõi b·∫Øt ƒë·∫ßu gi·∫£m theo scheduler.

# M·ªôt s·ªë platforms ph·ªï bi·∫øn tools & framework trong Llms

1. [Hugging Face](https://huggingface.co/): N·ªÅn t·∫£ng ph·ªï bi·∫øn cho c√°c m√¥ h√¨nh, b·ªô d·ªØ li·ªáu, b·∫£ng x·∫øp h·∫°ng v√† c·∫£ c√°c ·ª©ng d·ª•ng
2. [LangChain](https://www.langchain.com/) ‚Äì Framework m√£ ngu·ªìn m·ªü cung c·∫•p c√°c k·∫øt n·ªëi nhi·ªÅu thao t√°c v·ªõi M√¥ H√¨nh Ng√¥n Ng·ªØ L·ªõn (LLM) th√¥ng qua m·ªôt API ƒë∆°n gi·∫£n
3. [Gradio](https://www.gradio.app/) ‚Äì M·ªôt framework UI c·ª±c k·ª≥ ƒë∆°n gi·∫£n, cho ph√©p b·∫°n t·∫°o giao di·ªán ng∆∞·ªùi d√πng prototype ch·ªâ v·ªõi m·ªôt d√≤ng m√£, kh√¥ng c·∫ßn kinh nghi·ªám frontend.
4. Alternatives ‚Äì C√°c l·ª±a ch·ªçn thay th·∫ø bao g·ªìm Streamlit, Dash v√† Mesop t·ª´ Google
5. [Weights & Biases](https://wandb.ai/site) ‚Äì C√¥ng c·ª• ph√¢n t√≠ch v√† tr·ª±c quan h√≥a trong qu√° tr√¨nh hu·∫•n luy·ªán m√¥ h√¨nh.
6. [Google Colab](https://colab.research.google.com/) ‚Äì Vi·∫øt, ƒë√°nh gi√° v√† chia s·∫ª c√°c notebook t·ª´ xa tr√™n m·ªôt m√°y ch·ªß trong Google Cloud.
7. [Modal.com](https://modal.com/) ‚Äì N·ªÅn t·∫£ng AI kh√¥ng m√°y ch·ªß (serverless) d√†nh cho vi·ªác tri·ªÉn khai m√¥ h√¨nh.

**"Ngo√†i ra, n·∫øu b·∫°n mu·ªën h·ªçc s√¢u h∆°n v·ªÅ lƒ©nh v·ª±c n√†y, m√¨nh khuy·∫øn kh√≠ch c√°c b·∫°n tham gia c√°c kh√≥a h·ªçc uy t√≠n ƒë·ªÉ n·∫Øm v·ªØng n·ªÅn t·∫£ng tr∆∞·ªõc khi ƒëi s√¢u v√†o t·ª´ng kh√≠a c·∫°nh."

https://nvdam.widen.net/s/wlbgbqr7cj/nvidia-learning-training-course-catalog

Tai lieu tham khao trong video:
[Llm course](https://www.udemy.com/course/llm-engineering-master-ai-and-large-language-models/?srsltid=AfmBOorXXQrsRbJli2ye4GP3ICQCN5g5wUtofSmIx3ef6H6XqdbrC59P&couponCode=KEEPLEARNING)

    
